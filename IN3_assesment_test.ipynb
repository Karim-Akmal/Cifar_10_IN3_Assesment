{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvXC9IxIRhwE",
        "outputId": "5cac64b1-9b95-4a9f-8e17-112c4ce8a8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28575934.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define transformations to apply to the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),  # Randomly crop and resize the image to 32x32\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image pixel values\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-10 training dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Create DataLoader for validation set\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Download and load the CIFAR-10 test dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader for test set\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_dataset))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpalkTefwatR",
        "outputId": "d2997747-eb29-42bb-a33a-b24625073e5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QvIapecQA9Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Define the CNN model\n",
        "# class cifar_10(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(cifar_10, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu2 = nn.ReLU()\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu3 = nn.ReLU()\n",
        "#         self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "#         self.relu4 = nn.ReLU()\n",
        "#         self.pool4 = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "#         self.fc1 = nn.Linear(256, 128)\n",
        "#         self.relu5 = nn.ReLU()\n",
        "\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool1(self.relu1(self.conv1(x)))\n",
        "#         x = self.pool2(self.relu2(self.conv2(x)))\n",
        "#         x = self.pool3(self.relu3(self.conv3(x)))\n",
        "#         x = self.pool4(self.relu4(self.conv4(x)))\n",
        "#         # print(x.shape)\n",
        "#         x = x.view(-1, 256)\n",
        "#         # print(x.shape)\n",
        "#         x = self.relu5(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# model = cifar_10().to(device)\n",
        "\n",
        "# # Define the loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "CFVV8y5QA_wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the CNN model\n",
        "class cifar_10_modified(nn.Module):\n",
        "    def __init__(self, num_classes = 10, dropout_prob=0.5):\n",
        "        super(cifar_10_modified, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(256)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.batchnorm1(self.relu1(self.conv1(x))))\n",
        "        x = self.pool2(self.batchnorm2(self.relu2(self.conv2(x))))\n",
        "        x = self.pool3(self.batchnorm3(self.relu3(self.conv3(x))))\n",
        "        x = self.pool4(self.batchnorm4(self.relu4(self.conv4(x))))\n",
        "        # print(x.shape)\n",
        "        x = x.view(-1, 256)\n",
        "        # print(x.shape)\n",
        "        x = self.relu5(self.fc1(x))\n",
        "        # x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = cifar_10_modified().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "id": "f9TVkZyxc-qu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# model(input)"
      ],
      "metadata": {
        "id": "aEaZDTeRvRQy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "AVrz1lw3z4XI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images.to(device))\n",
        "        loss = criterion(outputs.to(device), labels.to(device))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            outputs = model(images.to(device))\n",
        "            loss = criterion(outputs.to(device), labels.to(device))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Training Loss: {loss:.4f}, '\n",
        "          f'Validation Loss: {val_loss / len(val_loader):.4f}, '\n",
        "          f'Validation Accuracy: {(100 * correct / total):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VEoM4LO9fqf",
        "outputId": "0357e261-692d-4efd-d3b9-37b5afcbaece"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Training Loss: 1.8674, Validation Loss: 1.7844, Validation Accuracy: 32.88%\n",
            "Epoch 2/50, Training Loss: 1.4540, Validation Loss: 1.6044, Validation Accuracy: 40.97%\n",
            "Epoch 3/50, Training Loss: 1.6227, Validation Loss: 1.5257, Validation Accuracy: 44.24%\n",
            "Epoch 4/50, Training Loss: 1.5115, Validation Loss: 1.4470, Validation Accuracy: 47.38%\n",
            "Epoch 5/50, Training Loss: 0.8834, Validation Loss: 1.3585, Validation Accuracy: 51.17%\n",
            "Epoch 6/50, Training Loss: 1.2820, Validation Loss: 1.3361, Validation Accuracy: 52.15%\n",
            "Epoch 7/50, Training Loss: 1.1451, Validation Loss: 1.2676, Validation Accuracy: 55.24%\n",
            "Epoch 8/50, Training Loss: 1.2426, Validation Loss: 1.2557, Validation Accuracy: 55.25%\n",
            "Epoch 9/50, Training Loss: 0.8871, Validation Loss: 1.1881, Validation Accuracy: 57.63%\n",
            "Epoch 10/50, Training Loss: 1.1358, Validation Loss: 1.1895, Validation Accuracy: 57.97%\n",
            "Epoch 11/50, Training Loss: 0.9615, Validation Loss: 1.1971, Validation Accuracy: 58.54%\n",
            "Epoch 12/50, Training Loss: 0.8591, Validation Loss: 1.1881, Validation Accuracy: 58.14%\n",
            "Epoch 13/50, Training Loss: 1.2997, Validation Loss: 1.1434, Validation Accuracy: 59.97%\n",
            "Epoch 14/50, Training Loss: 1.2034, Validation Loss: 1.1132, Validation Accuracy: 61.29%\n",
            "Epoch 15/50, Training Loss: 0.9955, Validation Loss: 1.0777, Validation Accuracy: 61.57%\n",
            "Epoch 16/50, Training Loss: 0.8664, Validation Loss: 1.0670, Validation Accuracy: 62.50%\n",
            "Epoch 17/50, Training Loss: 0.6239, Validation Loss: 1.0600, Validation Accuracy: 62.58%\n",
            "Epoch 18/50, Training Loss: 0.8073, Validation Loss: 1.0451, Validation Accuracy: 62.63%\n",
            "Epoch 19/50, Training Loss: 1.1396, Validation Loss: 1.0312, Validation Accuracy: 63.26%\n",
            "Epoch 20/50, Training Loss: 1.3109, Validation Loss: 1.0128, Validation Accuracy: 64.58%\n",
            "Epoch 21/50, Training Loss: 0.9087, Validation Loss: 1.0326, Validation Accuracy: 63.34%\n",
            "Epoch 22/50, Training Loss: 0.7271, Validation Loss: 1.0214, Validation Accuracy: 64.21%\n",
            "Epoch 23/50, Training Loss: 0.7880, Validation Loss: 0.9974, Validation Accuracy: 65.42%\n",
            "Epoch 24/50, Training Loss: 0.8267, Validation Loss: 1.0322, Validation Accuracy: 64.24%\n",
            "Epoch 25/50, Training Loss: 1.0606, Validation Loss: 0.9853, Validation Accuracy: 65.62%\n",
            "Epoch 26/50, Training Loss: 1.0580, Validation Loss: 1.0213, Validation Accuracy: 64.62%\n",
            "Epoch 27/50, Training Loss: 0.7654, Validation Loss: 0.9690, Validation Accuracy: 66.48%\n",
            "Epoch 28/50, Training Loss: 1.0386, Validation Loss: 0.9802, Validation Accuracy: 65.71%\n",
            "Epoch 29/50, Training Loss: 0.7044, Validation Loss: 0.9688, Validation Accuracy: 65.70%\n",
            "Epoch 30/50, Training Loss: 0.7783, Validation Loss: 0.9685, Validation Accuracy: 66.45%\n",
            "Epoch 31/50, Training Loss: 1.0460, Validation Loss: 0.9751, Validation Accuracy: 66.15%\n",
            "Epoch 32/50, Training Loss: 0.8176, Validation Loss: 0.9613, Validation Accuracy: 66.65%\n",
            "Epoch 33/50, Training Loss: 1.0319, Validation Loss: 0.9576, Validation Accuracy: 67.25%\n",
            "Epoch 34/50, Training Loss: 0.7564, Validation Loss: 0.9414, Validation Accuracy: 66.95%\n",
            "Epoch 35/50, Training Loss: 0.7463, Validation Loss: 0.9236, Validation Accuracy: 67.57%\n",
            "Epoch 36/50, Training Loss: 0.7618, Validation Loss: 0.9360, Validation Accuracy: 67.51%\n",
            "Epoch 37/50, Training Loss: 0.8240, Validation Loss: 0.9652, Validation Accuracy: 66.67%\n",
            "Epoch 38/50, Training Loss: 0.5205, Validation Loss: 0.9256, Validation Accuracy: 67.84%\n",
            "Epoch 39/50, Training Loss: 0.7254, Validation Loss: 0.9186, Validation Accuracy: 68.28%\n",
            "Epoch 40/50, Training Loss: 1.1728, Validation Loss: 0.9615, Validation Accuracy: 66.46%\n",
            "Epoch 41/50, Training Loss: 0.5533, Validation Loss: 0.9358, Validation Accuracy: 67.37%\n",
            "Epoch 42/50, Training Loss: 0.6491, Validation Loss: 0.9276, Validation Accuracy: 67.57%\n",
            "Epoch 43/50, Training Loss: 0.8157, Validation Loss: 0.9078, Validation Accuracy: 68.72%\n",
            "Epoch 44/50, Training Loss: 0.9425, Validation Loss: 0.9345, Validation Accuracy: 68.37%\n",
            "Epoch 45/50, Training Loss: 0.9393, Validation Loss: 0.9139, Validation Accuracy: 67.93%\n",
            "Epoch 46/50, Training Loss: 0.5270, Validation Loss: 0.9017, Validation Accuracy: 68.55%\n",
            "Epoch 47/50, Training Loss: 0.5008, Validation Loss: 0.8890, Validation Accuracy: 68.82%\n",
            "Epoch 48/50, Training Loss: 0.8547, Validation Loss: 0.9252, Validation Accuracy: 67.58%\n",
            "Epoch 49/50, Training Loss: 0.6461, Validation Loss: 0.9176, Validation Accuracy: 68.15%\n",
            "Epoch 50/50, Training Loss: 0.7772, Validation Loss: 0.9277, Validation Accuracy: 67.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/model_path/cifar_10.pth\")"
      ],
      "metadata": {
        "id": "BxlSKRa8_2q2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pnt2vQ0nLU4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VjOdS4FGBHVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLGbGoTeBHY3",
        "outputId": "eb747488-2700-4fea-9b90-dcd51f161251"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6807\n",
            "Precision: 0.6845\n",
            "Recall: 0.6807\n",
            "F1-Score: 0.6773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lszAKFiCBhE6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}